{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdBKHbKLcei-",
        "outputId": "01e9a9fa-a5fb-46d1-e30a-8ea2872a2dc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Sastrawi in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "#Import Librariy\n",
        "!pip install Sastrawi       # Menginstal library Sastrawi untuk stemming bahasa Indonesia\n",
        "!pip install nltk           # Menginstal library NLTK (Natural Language Toolkit) untuk pengolahan teks\n",
        "import pickle               # Modul untuk menyimpan dan memuat objek Python (seperti model)\n",
        "from pickle import UnpicklingError  # Untuk menangani error saat memuat file pickle\n",
        "import re                   # Modul regex untuk pembersihan teks (cleaning)\n",
        "import string               # Modul untuk manipulasi karakter seperti tanda baca\n",
        "from nltk.tokenize import word_tokenize       # Fungsi untuk memecah teks menjadi kata (tokenisasi)\n",
        "from nltk.corpus import stopwords             # Modul daftar stopwords (kata umum yang diabaikan)\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory  # Untuk membuat stemmer bahasa Indonesia\n",
        "import requests             # Digunakan untuk mengambil data dari URL (misalnya file JSON atau teks)\n",
        "import json                 # Untuk parsing data berformat JSON\n",
        "import nltk                 # Mengimpor modul utama NLTK\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mengunggah file svm_model.pkl dan tfidf_vectorizer.pkl untuk melkukan testing\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "xH1AJnr4e1Of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Kode ini bertujuan untuk melakukan prediksi sentimen dari satu kalimat ulasan baru menggunakan pipeline preprocessing dan model SVM + TF-IDF yang telah disimpan sebelumnya.\n",
        "try:\n",
        "    with open('/content/svm_model.pkl', 'rb') as f:\n",
        "        svm = pickle.load(f)\n",
        "except (EOFError, UnpicklingError) as e:\n",
        "    print(f\"Error loading xgb model: {e}\")\n",
        "\n",
        "    print(\"Check if 'svm_model.pkl' exists and is not empty.\")\n",
        "    print(\"If the file was transferred, ensure it was done in binary mode.\")\n",
        "\n",
        "\n",
        "try:\n",
        "    with open('/content/tfidf_vectorizer.pkl', 'rb') as f:\n",
        "        tfidf = pickle.load(f)\n",
        "except (EOFError, UnpicklingError) as e:\n",
        "    print(f\"Error loading TF-IDF vectorizer: {e}\")\n",
        "\n",
        "\n",
        "def cleaningText(text):\n",
        "    text = re.sub(r'@[A-Za-z0-9]+', '', text)\n",
        "    text = re.sub(r'#[A-Za-z0-9]+', '', text)\n",
        "    text = re.sub(r\"http\\S+\", '', text)\n",
        "    text = re.sub(r'[0-9]+', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = text.replace('\\n', ' ')\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = ' '.join([word for word in text.split() if word.lower() not in [\"mobile\", \"bni\", \"wondr\", \"bni mobile\"]])\n",
        "    text = text.strip(' ')\n",
        "    return text\n",
        "\n",
        "def casefoldingText(text):\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "def tokenizingText(text):\n",
        "    text = word_tokenize(text)\n",
        "    return text\n",
        "\n",
        "def filteringText(text):\n",
        "    listStopwords = set(stopwords.words('indonesian'))\n",
        "    listStopwords1 = set(stopwords.words('english'))\n",
        "    listStopwords.update(listStopwords1)\n",
        "    listStopwords.update(['iya','yaa','gak','nya','na','sih','ku','di','ya','loh','kah','deh'])\n",
        "    filtered = []\n",
        "    for txt in text:\n",
        "        if txt not in listStopwords:\n",
        "            filtered.append(txt)\n",
        "    text = filtered\n",
        "    return text\n",
        "\n",
        "def stemmingText(text):\n",
        "    factory = StemmerFactory()\n",
        "    stemmer = factory.create_stemmer()\n",
        "    words = text.split()\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]\n",
        "    stemmed_text = ' '.join(stemmed_words)\n",
        "    return stemmed_text\n",
        "\n",
        "def toSentence(list_words):\n",
        "    sentence = ' '.join(word for word in list_words)\n",
        "    return sentence\n",
        "\n",
        "def fix_slangwords(text):\n",
        "    words = text.split()\n",
        "    fixed_words = []\n",
        "    for word in words:\n",
        "        if word.lower() in slangwords:\n",
        "            fixed_words.append(slangwords[word.lower()])\n",
        "        else:\n",
        "            fixed_words.append(word)\n",
        "    fixed_text = ' '.join(fixed_words)\n",
        "    return fixed_text\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/Nicholas1B/ProjectUAS/refs/heads/main/slangword.json'  # URL tempat kamus slangwords disimpan\n",
        "\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    try:\n",
        "        slangwords = json.loads(response.text)\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(\"Error decoding JSON:\", e)\n",
        "        print(\"Response content:\", response.text)\n",
        "else:\n",
        "    print(\"Failed to fetch data from URL. Status code:\", response.status_code)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = cleaningText(text)\n",
        "    text = casefoldingText(text)\n",
        "    text = fix_slangwords(text)\n",
        "    text = tokenizingText(text)\n",
        "    text = filteringText(text)\n",
        "    text = toSentence(text)\n",
        "    return text\n",
        "\n",
        "def prediksi_sentimen_kalimat_baru(review_baru, tfidf, svm):\n",
        "\n",
        "    review_baru_cleaned = cleaningText(review_baru)\n",
        "    review_baru_casefolded = casefoldingText(review_baru_cleaned)\n",
        "    review_baru_slangfixed = fix_slangwords(review_baru_casefolded)\n",
        "    review_baru_tokenized = tokenizingText(review_baru_slangfixed)\n",
        "    review_baru_filtered = filteringText(review_baru_tokenized)\n",
        "    review_baru_final = toSentence(review_baru_filtered)\n",
        "\n",
        "\n",
        "    X_review_baru = tfidf.transform([review_baru_final])\n",
        "\n",
        "\n",
        "    X_review_baru = X_review_baru.toarray()\n",
        "\n",
        "\n",
        "    prediksi_sentimen = svm.predict(X_review_baru)\n",
        "\n",
        "\n",
        "    if prediksi_sentimen[0] == 'positive':\n",
        "        hasil = \"Sentimen review baru adalah POSITIF.\"\n",
        "    elif prediksi_sentimen[0] == 'negative':\n",
        "        hasil = \"Sentimen review baru adalah NEGATIF.\"\n",
        "    else:\n",
        "        hasil = \"Sentimen review baru adalah NETRAL.\"\n",
        "\n",
        "    return hasil"
      ],
      "metadata": {
        "id": "3anT7tqvgRAp"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sentimen negatif\n",
        "review_baru = \"sering eror\"\n",
        "prediksi_sentimen_kalimat_baru(review_baru, tfidf, svm)"
      ],
      "metadata": {
        "id": "bJ2tIoEPgidV",
        "outputId": "c6647b6d-c76a-425b-95c0-6c0a8a083731",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sentimen review baru adalah NEGATIF.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sentimen positif\n",
        "review_baru = \" efek foto, filternya\"\n",
        "prediksi_sentimen_kalimat_baru(review_baru, tfidf, svm)"
      ],
      "metadata": {
        "id": "EQ96GQyZhah-",
        "outputId": "e0e3dcaf-e6f9-4ea3-b49a-2ecc639fcd50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sentimen review baru adalah POSITIF.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sentimen Netral\n",
        "review_baru = \"Aplikasi standar untuk berbagi foto dan video\"\n",
        "prediksi_sentimen_kalimat_baru(review_baru, tfidf, svm)"
      ],
      "metadata": {
        "id": "rAB1OFmjhuWJ",
        "outputId": "baed019c-5439-489a-c0b1-633bb12b6945",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sentimen review baru adalah POSITIF.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    }
  ]
}